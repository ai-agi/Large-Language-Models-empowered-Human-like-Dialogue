# **Miscellaneous Dialog Datum**:blush:  

___  
## __Note__  
$\color{green}{Paper}$ :dart:  
No star if cite number is below 10;  one stars if cite number is below 100;  two stars if cite number is below 1000; three stars if cite number is below 5000; four stars if cite number is below 10000;  five stars if cite number exceed 10000. For example, DialoGPT have been cited 892 times (before April 28th, 2023) and it should be marked as two stars. 

$\color{orange}{Dataset \ or \ Code\}$ :dart:  
No star if github or gitee stars is below 100;  one stars if github or gitee stars are below 1K;  two stars if github or gitee stars is below 10K; three stars if github or gitee stars are below 50K; four stars if github or gitee stars are below 100K;  five stars if github or gitee stars exceed 1M. For example, https://github.com/openai/chatgpt-retrieval-plugin has 15.2K stars (before April 28th, 2023) and it should be marked as three stars.  

___  
## Open Domain Dialog  (Shen Fanqi)  
* ### _Paper_  
  * #### Retrieval Model  
    * [M2GCN: multi-modal graph convolutional network for modeling polypharmacy](https://link.springer.com/article/10.1007/s10489-022-03839-z)
    * [M2GNN: Metapath and Multi-interest Aggregated Graph Neural Network for Tag-based Cross-domain Recommendation](https://arxiv.org/pdf/2304.07911.pdf)
  * #### Generative Model   
    * [DialoGPT(ACL2020)--DIALOGPT:Large-Scale Generative Pre-training for Conversational Response Generation] 
     (https://arxiv.org/abs/1911.00536) :star::star:   
    * [FusedChat--Fusing Task-Oriented and Open-Domain Dialogues in Conversational Agents] 2022-06-28
     (https://ojs.aaai.org/index.php/AAAI/article/view/21416) :star:
    * [EVA2.0--EVA2.0: Investigating Open-domain Chinese Dialogue Systems with Large-scale Pre-training] 2023
     (https://link.springer.com/article/10.1007/s11633-022-1387-3) :star:
    * [HOPE--Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations] 2022-02
     (https://dl.acm.org/doi/abs/10.1145/3488560.3498509) :star:
    * [TOD turnsTOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue] 2020
     (https://arxiv.org/abs/2004.06871) :star::star:
    * [LAION-400_2021--LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs] 2021
     (https://arxiv.org/abs/2111.02114) :star::star:
    * [Crosswoz--CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset] 2020 
     (https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00314/96453/CrossWOZ-A-Large-Scale-Chinese-Cross-Domain-Task) :star:
    * [Search-engine-augmented dialogue response generation with cheaply supervised query production](https://arxiv.org/pdf/2302.09300.pdf) 2023-04
  * #### ![#1589F0](https://via.placeholder.com/15/1589F0/000000?text=+) ![##FF6347](https://via.placeholder.com/15/ff6347/000000?text=+) ![##FFD700](https://via.placeholder.com/15/ffd700/000000?text=+) `Multi-modal`
    * [MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation](https://arxiv.org/pdf/2211.05719.pdf)
    * [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://proceedings.mlr.press/v162/li22n/li22n.pdf):star::star: [2022]
    
* ### _Dataset_   
  * #### Retrieval Model  
    * [M2GCN](https://github.com/farkguidao/M2GCN)[2021-12]
  * #### Generative Model  
    * [DialoGPT](https://github.com/huggingface/transformers):star::star::star::star:
    * [FusedChat](https://github.com/tomyoung903/FusedChat) [2022]
    * [EVA2.0](https://github.com/thu-coai/EVA):star:
    * [HOPE](https://github.com/LCS2-IIITD/SPARTA_WSDM2022) [2022]
    * [TOD turns](https://github.com/tomyoung903/FusedChat) [2022]
    * [CDial-GPT](https://github.com/thu-coai/CDial-GPT):star::star: [2022]
    * [zhddline](https://github.com/zll17/Neural_Topic_Models):star: [2022]
    * [CrossWOZ](https://github.com/thu-coai/CrossWOZ):star: [2023-01]
    * [banjori](https://github.com/baderj/domain_generation_algorithms):star: [2023-03]
  * #### ![#1589F0](https://via.placeholder.com/15/1589F0/000000?text=+) ![##FF6347](https://via.placeholder.com/15/ff6347/000000?text=+) ![##FFD700](https://via.placeholder.com/15/ffd700/000000?text=+) `Multi-modal`
    * [MMDialog](https://github.com/victorsungo/MMDialog):star: [2022-12]
    [VISDIAL](https://paperswithcode.com/dataset/visdial)
    * [BLIP](https://github.com/salesforce/BLIP):star::star: [2022-09]
    * [lavis](https://github.com/salesforce/lavis):star::star:
    
* ### _Code_   
  * #### Retrieval Model  
  * #### Generative Model 

***  
## Task-oriented Dialog  (Jiang Yi)  
* ### Paper  
  * #### Understanding  
    * [Robustness testing of language understanding in task-oriented dialog](https://arxiv.org/abs/2012.15262):star:
    * [SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding](https://arxiv.org/abs/2209.06638):star:
    * [Space-3: Unified dialog model pre-training for task-oriented dialog understanding and generation](https://arxiv.org/abs/2209.06664)
    * [Natural language understanding for task oriented dialog in the biomedical domain in a low resources context](https://arxiv.org/abs/1811.09417)
    * [Proactive Human-Machine Conversation with Explicit Conversation Goals](https://arxiv.org/abs/1906.05572)
  * #### Generation 
    * [Continual learning for natural language generation in task-oriented dialog systems](https://arxiv.org/abs/2010.00910):star:  
    * [Efficient retrieval augmented generation from unstructured knowledge for task-oriented dialog](https://arxiv.org/abs/2102.04643):star:
    * [Galaxy: A generative pre-trained model for task-oriented dialog with semi-supervised learning and explicit policy injection](https://ojs.aaai.org/index.php/AAAI/article/view/21320):star:
    * [Recent advances and challenges in task-oriented dialog systems](https://link.springer.com/article/10.1007/s11431-020-1692-3):star::star:
    * [PRAL: A tailored pre-training model for task-oriented dialog generation](https://aclanthology.org/2021.acl-short.40/):star:
    * [MuTual: A Dataset for Multi-Turn Dialogue Reasoning](https://paperswithcode.com/paper/mutual-a-dataset-for-multi-turn-dialogue):star:
    * [NaturalConv: A Chinese Dialogue Dataset Towards Multi-turn Topic-driven Conversation](https://arxiv.org/pdf/2103.02548.pdf)
  * #### ![#1589F0](https://via.placeholder.com/15/1589F0/000000?text=+) ![##FF6347](https://via.placeholder.com/15/ff6347/000000?text=+) ![##FFD700](https://via.placeholder.com/15/ffd700/000000?text=+) `Multi-modal`  
    * [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://paperswithcode.com/paper/mvp-multi-task-supervised-pre-training-for):star::star::star: 
* ### Dataset  
  * #### Understanding  
    * [Snips](https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines):star::star::star::star: 
    * [MIT Restaurant Corpus](https://groups.csail.mit.edu/sls/downloads/restaurant/):star::star:
    * [MIT Movie Corpus](https://groups.csail.mit.edu/sls/downloads/movie/):star::star::star:
    * [frames](https://datasets.maluuba.com/Frames/dl):star::star::star:
    * [WOZ](https://paperswithcode.com/paper/multiwoz-a-large-scale-multi-domain-wizard-of):star::star::star:
    * [DuConv](https://link.zhihu.com/?target=https%3A//github.com/Cindy-xdZhang/ACL-duconv)
  * #### Generation 
    * [including papers in README](https://github.com/songsonggithub/Task-Oriented-Dialogue-Dataset-Survey) 
    * [Dialogue State Tracking](https://github.com/AtmaHou/Task-Oriented-Dialogue-Research-Progress-Survey):star:
    * [MuTual](https://paperswithcode.com/paper/mutual-a-dataset-for-multi-turn-dialogue):star::star::star:
    * [Schema-Guided Dialogue (SGD)](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue):star::star:
    * [Wizard of Wikipedia - ICLR 2019](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/ParlAI/tree/master/projects/wizard_of_wikipedia2)
    * [Movie-chats - EMNLP 2020](https://link.zhihu.com/?target=https%3A//github.com/chin-gyou/MovieChats)
    * [doc2dial - ACL 2021](https://link.zhihu.com/?target=https%3A//github.com/doc2dial/sharedtask-dialdoc2021)
    * [NaturalConv - 2021](https://link.zhihu.com/?target=https%3A//ai.tencent.com/ailab/nlp/dialogue/%23datasets)
  * #### ![#1589F0](https://via.placeholder.com/15/1589F0/000000?text=+) ![##FF6347](https://via.placeholder.com/15/ff6347/000000?text=+) ![##FFD700](https://via.placeholder.com/15/ffd700/000000?text=+) `Multi-modal` 
    * [Multimodal Dialogs (MMD) Dataset](https://github.com/AtmaHou/Task-Oriented-Dialogue-Research-Progress-Survey):star: 
    * [Facebook Multilingual Task Oriented Dataset](https://fb.me/multilingual_task_oriented_data):star::star::star:
    * [MMD: Towards Building Large Scale Multimodal Domain-Aware Conversation Systems(https://amritasaha1812.github.io/MMD/)]  
      (https://github.com/amritasaha1812/MMD_Code)
 
  
***  
## Question Answering  (Chen Pu)  
* ### Paper  
  * #### Understanding  
    * ["TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue"](https://aclanthology.org/2020.emnlp-main.66.pdf)
    * [Soloist: Building task bots at scale with transfer learning and machine teaching. TACL 2021](https://github.com/pengbaolin/soloist)
    * [UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2. AAAI 2021](https://github.com/TonyNemo/UBAR-MultiWOZ)
    * [Unified Dialog Model Pre-training for Task-Oriented Dialog Understanding and Generation](https://dl.acm.org/doi/10.1145/3477495.3532069)
    * [Task-Oriented Dialogue System as Natural Language Generation](https://dl.acm.org/doi/10.1145/3477495.3531920)
  * #### Generation  
    * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
    * [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
  * #### ![#1589F0](https://via.placeholder.com/15/1589F0/000000?text=+) ![##FF6347](https://via.placeholder.com/15/ff6347/000000?text=+) ![##FFD700](https://via.placeholder.com/15/ffd700/000000?text=+) `Multi-modal` 
    * [MiniGPT-4](https://minigpt-4.github.io/)
    * [MMCoQA: Conversational Question Answering over Text, Tables, and Images](https://aclanthology.org/2022.acl-long.290/)
    * [Understanding User Satisfaction with Task-oriented Dialogue Systems](https://dl.acm.org/doi/10.1145/3477495.3531798)
* ### Dataset  
  * #### Understanding  
    * [DSTC2-3](https://github.com/matthen/dstc)
    * [SGD](https://ojs.aaai.org//index.php/AAAI/article/view/6394)
    * [SMD](https://pubmed.ncbi.nlm.nih.gov/11125075/)
    * [MSR](https://msropendata.com/datasets/14e910fa-af4a-4a6a-b3b5-fef2f688c923)
    * [Self-play](https://aclanthology.org/N18-1187.pdf)
    * [frames](https://github.com/Maluuba/frames)
    * [VQA](https://visualqa.org/download.html)
    * [ArchivalQA: A Large-scale Benchmark Dataset for Open-Domain Question Answering over Historical News Collections](https://dl.acm.org/doi/10.1145/3477495.3531734)
    * [VideoABC: A Real-World Video Dataset for Abductive Visual Reasoning](https://ieeexplore.ieee.org/document/9893026)
  * #### Generation 
    * [MultiWOZ 2.0](https://aclanthology.org/D18-1547/) 
    * [MultiWOZ 2.1](https://arxiv.org/abs/1907.01669)
    * [MultiWOZ 2.2](https://aclanthology.org/2020.nlp4convai-1.13/)
  * #### ![#1589F0](https://via.placeholder.com/15/1589F0/000000?text=+) ![##FF6347](https://via.placeholder.com/15/ff6347/000000?text=+) ![##FFD700](https://via.placeholder.com/15/ffd700/000000?text=+) `Multi-modal`

*** 
## Dialogue & Question Answering Evaluator
* ### Generative Model
  * #### Paper  
    * #####  
    * ##### 
  * #### Model

  * #### Metrics
    * ##### Correlation between Automated Evaluation Scores and Human Judgement
       * ###### Spearman Correlation
       * ###### Pearson Correlation


  * #### Human Evaluation

  * #### Automated Evaluation Metrics

  * #### ![#1589F0](https://via.placeholder.com/15/1589F0/000000?text=+) ![##FF6347](https://via.placeholder.com/15/ff6347/000000?text=+) ![##FFD700](https://via.placeholder.com/15/ffd700/000000?text=+) `Multi-modal`

  * #### <font color=#1589F0 >Multi</font> <font color=##FF6347 >-</font> <font color=##FFD700 >modal</font>

    
***

<div align=center>
<img src="http://pic11.photophoto.cn/20090626/0036036341009653_b.jpg" width="50%" height="50%">
</div>
